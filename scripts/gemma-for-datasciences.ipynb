{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8018995,"sourceType":"datasetVersion","datasetId":4724972},{"sourceId":11270,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":6216}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# gemma-2b AS A DATA SCIENCE TEACHER\n\n## 100% LOCAL, WITHOUT FINETUNING, WITH _YOUR OWN DATA_\n\n> _Information is the oil of the 21st century, and analytics is the combustion engine â€“ Peter Sondergaard (Senior Vice President and the Global Head of Research at Gartner Inc)_\n\nIn a world where data are becoming more important with each day passing, data science is a fundamental discipline to master in order to understand and solve the upcoming challenges of the Big Data World.\n\nUnfortunately, data science is generally available to University-level students only, making it difficult for other people to access its concepts. This obstacle can be removed with the help of Large Language Models, such as _gemma-2b_.\n\nIn this notebook, we'll make our way through the jungle of data science thanks to _gemma-2b_, a simple pdf file titled **\"What is data science?\"** <a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1), ChromaDB vectorstores and Langchain, all elengatly written in python.\n\nThe final goal is to implement a simple, yet powerful, pipeline to generate a 100% local and fully-customizable LLM-based assistant that works with the user's data.\n\nLet's dive in!ðŸ›«\n\n\n<a name=\"cite_note-1\"></a>[<sup>[1]</sup>](#cite_ref-1) Brodie, Michael. (2019). What Is Data Science?. 10.1007/978-3-030-11821-1_8.","metadata":{"id":"cY7PiUMxHqWO"}},{"cell_type":"markdown","source":"# Build the environment\n\nFirst of all, we want everything set up the right way to work properly. To do so, we need to:\n\n1. Upload the pdf file in our workspace (we can simply create a dataset in Kaggle containing the pdf and add it as `input` to the notebook): in the following notebook example, we will name it \"/kaggle/input/what-is-datascience-docs/WhatisDataScienceFinalMay162018.pdf\". \n2. Install necessary dependencies\n3. Upload _gemma-2b_ model as Kaggle input\n4. Define useful functions to make our LLM-based data science assistant work","metadata":{"id":"YpQRNdd3NDT0"}},{"cell_type":"code","source":"# INSTALL NECESSARY DEPENDENCIES\n\n## Versions provided for the packages are not strict... Still, you may encounter issues if you use different ones\n\n! python3 -m pip install langchain-community==0.0.13 langchain==0.1.1 torch==2.1.2","metadata":{"id":"mTFqzkbzPQ5o","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# INSTALL NECESSARY DEPENDENCIES (pt2)\n\n! python3 -m pip install trl peft","metadata":{"id":"qb2GLlQZZan3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# INSTALL NECESSARY DEPENDENCIES (pt3)\n\n! pip install pypdf==3.17.4","metadata":{"id":"GV6gIH8ucztn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# INSTALL NECESSARY DEPENDENCIES (pt4)\n\n! pip install sentence_transformers==2.2.2","metadata":{"id":"lyEB5Rc_dOec","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# INSTALL NECESSARY DEPENDENCIES (pt6)\n\n! pip install chromadb==0.4.22","metadata":{"id":"B0onWX8heNch","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IMPORT gemma-2b MODEL FROM KAGGLE\n\n## To import the model, we'll be uploading the model directly from Kaggle input\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_checkpoint = \"/kaggle/input/gemma/transformers/2b/1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, token=hf_token)\nmodel = AutoModelForCausalLM.from_pretrained(model_checkpoint, token=hf_token)","metadata":{"id":"lcy5ImzmSLcq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DEFINE USEFUL FUNCTIONS\n\n## To chat, we'll need to create a vectorized database from our pdf and then build\n## a retrieval Q&A chain\n\nimport time\nfrom langchain_community.llms import HuggingFacePipeline\nfrom langchain.storage import LocalFileStore\nfrom langchain.embeddings import CacheBackedEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import ConversationalRetrievalChain\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nimport os\n\ndef create_a_persistent_db(pdfpath, dbpath, cachepath) -> None:\n    \"\"\"\n    Creates a persistent database from a PDF file.\n\n    Args:\n        pdfpath (str): The path to the PDF file.\n        dbpath (str): The path to the storage folder for the persistent LocalDB.\n        cachepath (str): The path to the storage folder for the embeddings cache.\n    \"\"\"\n    print(\"Started the operation...\")\n    a = time.time()\n    loader = PyPDFLoader(pdfpath)\n    documents = loader.load()\n\n    ### Split the documents into smaller chunks for processing\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n    texts = text_splitter.split_documents(documents)\n\n    ### Use HuggingFace embeddings for transforming text into numerical vectors\n    ### This operation can take a while the first time but, once you created your local database with\n    ### cached embeddings, it should be a matter of seconds to load them!\n    embeddings = HuggingFaceEmbeddings()\n    store = LocalFileStore(\n        os.path.join(\n            cachepath, os.path.basename(pdfpath).split(\".\")[0] + \"_cache\"\n        )\n    )\n    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n        underlying_embeddings=embeddings,\n        document_embedding_cache=store,\n        namespace=os.path.basename(pdfpath).split(\".\")[0],\n    )\n\n    b = time.time()\n    print(\n        f\"Embeddings successfully created and stored at {os.path.join(cachepath, os.path.basename(pdfpath).split('.')[0]+'_cache')} under namespace: {os.path.basename(pdfpath).split('.')[0]}\"\n    )\n    print(f\"To load and embed, it took: {b - a}\")\n\n    persist_directory = os.path.join(\n        dbpath, os.path.basename(pdfpath).split(\".\")[0] + \"_localDB\"\n    )\n    vectordb = Chroma.from_documents(\n        documents=texts,\n        embedding=cached_embeddings,\n        persist_directory=persist_directory,\n    )\n    c = time.time()\n    print(\n        f\"Persistent database successfully created and stored at {os.path.join(dbpath, os.path.basename(pdfpath).split('.')[0] + '_localDB')}\"\n    )\n    print(f\"To create a persistent database, it took: {c - b}\")\n    return vectordb\n\ndef just_chatting(\n    model,\n    tokenizer,\n    query,\n    vectordb,\n    chat_history=[]\n):\n    \"\"\"\n    Implements a chat system using Hugging Face models and a persistent database.\n\n    Args:\n        model (AutoModelForCausalLM): Hugging Face model, already loaded and prepared.\n        tokenizer (AutoTokenizer): Hugging Face tokenizer, already loaded and prepared.\n        model_task (str): Task for the Hugging Face model.\n        persistent_db_dir (str): Directory for the persistent database.\n        embeddings_cache (str): Path to cache Hugging Face embeddings.\n        pdfpath (str): Path to the PDF file.\n        query (str): Question by the user\n        vectordb (ChromaDB): vectorstorer variable for retrieval.\n        chat_history (list): A list with previous questions and answers, serves as context; by default it is empty (it may make the model allucinate)\n    \"\"\"\n    ### Create a text-generation pipeline and connect it to a ConversationalRetrievalChain\n    pipe = pipeline(\"text-generation\",\n                    model=model,\n                    tokenizer=tokenizer,\n                    max_new_tokens = 2048,\n                    repetition_penalty = float(10),\n    )\n\n    local_llm = HuggingFacePipeline(pipeline=pipe)\n    llm_chain = ConversationalRetrievalChain.from_llm(\n        llm=local_llm,\n        chain_type=\"stuff\",\n        retriever=vectordb.as_retriever(search_kwargs={\"k\": 1}),\n        return_source_documents=False,\n    )\n    rst = llm_chain({\"question\": query, \"chat_history\": chat_history})\n    return rst","metadata":{"id":"_8Tt0dtkgEfv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Chat with the model\n\nTo chat with the model, we first have to build our local, persistent, database, and also compute embeddings: after that, we'll be able to chat with the model without problems!ðŸš€","metadata":{"id":"pospjXN1a3lW"}},{"cell_type":"code","source":"# CREATE PERSISTENT DB\n\nfilepath = \"/kaggle/input/what-is-datascience-docs/WhatisDataScienceFinalMay162018.pdf\"\ndbpath = \"/kaggle/working/\"\ncachepath = \"/kaggle/working/\"\nvectordb = create_a_persistent_db(filepath, dbpath, cachepath)","metadata":{"id":"SSd1jia8bz5s","outputId":"ecc4c317-51f9-4cd3-a589-3138ccc39d23","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CHAT WITH MODEL\n\nchat_history = []\nquery = \"Define datascience\"\nres = just_chatting(model, tokenizer, query, vectordb, chat_history=chat_history)\nchat_history.append([query, res[\"answer\"].replace(\"\\n\",\" \")])","metadata":{"id":"LxJHt3LneuGD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\" \".join[res[\"answer\"]])","metadata":{"id":"utFaudIyitNO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implement a simple chat GUI (local only)\n\nWant to interact more directly with your model, without going through that pythonic stuff? Let's implement a very simple and rudimental chat GUI, based on builtin package `tkinter`, to achieve this goal!ðŸ¤¯","metadata":{"id":"JlmAbwghlVrO"}},{"cell_type":"code","source":"import tkinter as tk\nfrom tkinter import scrolledtext\n\nclass ChatGUI:\n    def __init__(self, master):\n        self.master = master\n        master.title(\"DataScienceAI\")\n\n        self.chat_history = scrolledtext.ScrolledText(master, wrap=tk.WORD, width=40, height=15)\n        self.chat_history.pack(padx=10, pady=10)\n\n        self.user_input = tk.Entry(master, width=40)\n        self.user_input.pack(padx=10, pady=10)\n\n        self.send_button = tk.Button(master, text=\"Send\", command=self.send_message)\n        self.send_button.pack(pady=10)\n\n        # Set up initial conversation\n        self.display_message(\"DataScienceAI: Hello! How can I help you today?\")\n\n    def send_message(self):\n        user_message = self.user_input.get()\n        self.display_message(f\"You: {user_message}\")\n        # Replace the next line with your chatbot logic to get a response\n        chatbot_response = f\"DataScienceAI: {just_chatting(model, tokenizer, user_message, vectordb)[\"answer\"].replace(\"\\n\",\" \")}\"\n        self.display_message(chatbot_response)\n        self.user_input.delete(0, tk.END)  # Clear the input field\n\n    def display_message(self, message):\n        self.chat_history.insert(tk.END, message + '\\n')\n        self.chat_history.see(tk.END)  # Scroll to the bottom\n\nif __name__ == \"__main__\":\n    root = tk.Tk()\n    chat_gui = ChatGUI(root)\n    root.mainloop()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions\n\nThis is it!\n\nWe built a simple assistant, fully customizable in terms of both the LLM employed (you can switch to _gemma-7b_ or to your favorite LLM) and the data you can make it work with (in this case is data sciences, but you can make it work also on a pdf about pallas' cats, if you want!)ðŸˆ.\n\nAnother important thing to note is that all of this is completely local, there is no need for hosted APIs, pay-as-you-go services or other things like that: everything is free to use, on your Desktop!\n\nThere are two main disadvantages in this approach: \n\n1. Performance-critical tasks, such as loading the model and making prediction, are heavily resource-dependent: to load big models (>1~2 GB) and to make them generate text, it is useful to have more than 16GB RAM and more than 4 CPU cores.\n2. Small (and old) models, such as _openai-community/gpt2_, can easily allucinate while generating text. This is generally prompt-dependent (meaning that they tend to produce trashy results on certain prompts more frequently than on other ones) and the issue almost totally resolves when employing large LLMs (_gemma-7b_ or _llama-7b_ would not-so-easily allucinate, for instance).\n\n### TLDRðŸ˜µ:\n\n**Pros**:\n- Simple and customizable\n- Use virtually any LLM you want\n- Use your own data\n- 100% local, 100% free, no payments or APIs\n\n**Cons**:\n- Performance might be resource-dependent for large LLMs (if you have >16GB RAM and >4 cores it shouldn't be a great problem)\n- Small LLMs can still allucinate","metadata":{"id":"hRy5ErJ_mkfV"}},{"cell_type":"markdown","source":"# References\n\n- Paul Mooney, Ashley Chow. (2024). Google â€“ AI Assistants for Data Tasks with Gemma. Kaggle. https://kaggle.com/competitions/data-assistants-with-gemma\n- Brodie, Michael. (2019). What Is Data Science?. 10.1007/978-3-030-11821-1_8.\n","metadata":{"id":"wHWCmEp9oFsC"}}]}